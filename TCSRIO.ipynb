{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMROuT9sFYoT/h6SGJOM0HY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TrilokGuru/TCSRIO/blob/main/TCSRIO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmimI5DZCH5C"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary to store each form ID and its writer\n",
        "from google.colab import drive\n",
        "import os\n",
        "from itertools import islice\n",
        "drive.mount(\"/content/gdrive\",force_remount=True)\n",
        "form_writer = {}\n",
        "with open('gdrive/My Drive/TCS RIO/forms.txt') as f:\n",
        "  for line in islice(f, 16, None):\n",
        "    line_list = line.split(' ')\n",
        "    form_id = line_list[0]\n",
        "    writer = line_list[1]\n",
        "    form_writer[form_id] = writer\n",
        "list(form_writer.items())[0:5]\n",
        "\n",
        "\n",
        "# Select the 50 most common writer\n",
        "from collections import Counter\n",
        "top_writers = []\n",
        "num_writers = 50\n",
        "writers_counter = Counter(form_writer.values())\n",
        "for writer_id,_ in writers_counter.most_common(num_writers):\n",
        "    top_writers.append(writer_id)\n",
        "print(top_writers[0:5])\n",
        "\n",
        "\n",
        "top_forms = []\n",
        "for form_id, author_id in form_writer.items():\n",
        "    if author_id in top_writers:\n",
        "        top_forms.append(form_id)\n",
        "print(top_forms[0:5])\n",
        "\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "tmp = []\n",
        "target_list = []\n",
        "path_to_files = os.path.join('gdrive/My Drive/TCS RIO/temp_sentences', '*')\n",
        "for filename in sorted(glob.glob(path_to_files)):\n",
        "  tmp.append(filename)\n",
        "  image_name = filename.split('/')[-1]\n",
        "  file, ext = os.path.splitext(image_name)\n",
        "  parts = file.split('-')\n",
        "  form = parts[0] + '-' + parts[1]\n",
        "  for key in form_writer:\n",
        "    if key == form:\n",
        "      target_list.append(str(form_writer[form]))\n",
        "img_files = np.array(tmp)\n",
        "img_targets = np.asarray(target_list)\n",
        "print(img_files[0:5])\n",
        "print(img_targets[0:5])\n",
        "print(img_files.shape)\n",
        "print(img_targets.shape)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "for file_name in img_files[:2]:\n",
        "    img = mpimg.imread(file_name)\n",
        "    plt.figure(figsize = (10,10))\n",
        "    plt.imshow(img, cmap ='gray')\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(img_targets)\n",
        "encoded_img_targets = encoder.transform(img_targets)\n",
        "print(\"Writer ID        : \", img_targets[:2])\n",
        "print(\"Encoded writer ID: \", encoded_img_targets[:2])\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(img_files, encoded_img_targets, test_size=0.2, shuffle = True)\n",
        "# Further split training set into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle = True)\n",
        "print(X_train.shape, X_val.shape, X_test.shape)\n",
        "print(y_train.shape, y_val.shape, y_test.shape)\n",
        "\n",
        "\n",
        "CROP_SIZE = 113\n",
        "NUM_LABELS = 50\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "\n",
        "#pip install pillow\n",
        "from sklearn.utils import shuffle\n",
        "from PIL import Image\n",
        "import random\n",
        "def get_augmented_sample(sample, label, sample_ratio):\n",
        "    # Get current image details\n",
        "    img = Image.open(sample)\n",
        "    img_width = img.size[0]\n",
        "    img_height = img.size[1]\n",
        "    # Compute resize dimensions such that aspect ratio is maintained\n",
        "    height_fac = CROP_SIZE / img_height\n",
        "    size = (int(img_width * height_fac), CROP_SIZE)\n",
        "    # Resize image \n",
        "    new_img = img.resize((size), Image.ANTIALIAS)\n",
        "    new_img_width = new_img.size[0]\n",
        "    new_img_height = new_img.size[1]\n",
        "    # Generate a random number of crops of size 113x113 from the resized image\n",
        "    x_coord = list(range(0, new_img_width - CROP_SIZE))\n",
        "    num_crops = int(len(x_coord) * sample_ratio)\n",
        "    random_x_coord = random.sample(x_coord, num_crops)\n",
        "    # Create augmented images (cropped forms) and map them to a label (writer)\n",
        "    images = []\n",
        "    labels = []\n",
        "    for x in random_x_coord:\n",
        "        img_crop = new_img.crop((x, 0, x + CROP_SIZE, CROP_SIZE))\n",
        "        # Transform image to an array of numbers\n",
        "        images.append(np.asarray(img_crop))\n",
        "        labels.append(label)\n",
        "    return (images, labels)\n",
        "sample, label = X_train[0], y_train[0]\n",
        "img = mpimg.imread(sample)\n",
        "plt.figure(figsize = (10,10))\n",
        "plt.imshow(img, cmap ='gray')\n",
        "print(\"Label: \", label)\n",
        "\n",
        "\n",
        "images, labels = get_augmented_sample(sample, label, 0.1)\n",
        "print(labels)\n",
        "print(\"Num of labels: \", len(labels))\n",
        "\n",
        "\n",
        "print(len(images))\n",
        "plt.imshow(images[0], cmap ='gray');\n",
        "plt.imshow(images[1], cmap ='gray');\n",
        "\n",
        "\n",
        "import operator\n",
        "from functools import reduce\n",
        "from keras.utils import to_categorical\n",
        "def generate_data(samples, labels, batch_size, sample_ratio):\n",
        "    while 1: \n",
        "        for offset in range(0, len(samples), batch_size):\n",
        "            batch_samples = samples[offset:(offset + batch_size)]\n",
        "            batch_labels = labels[offset:(offset + batch_size)]     \n",
        "            # Augment each sample in batch\n",
        "            augmented_batch_samples = []\n",
        "            augmented_batch_labels = []\n",
        "            for i in range(len(batch_samples)):\n",
        "                sample = batch_samples[i]\n",
        "                label = batch_labels[i]\n",
        "                augmented_samples, augmented_labels = get_augmented_sample(sample, label, sample_ratio)\n",
        "                augmented_batch_samples.append(augmented_samples)\n",
        "                augmented_batch_labels.append(augmented_labels)\n",
        "            # Flatten out samples and labels\n",
        "            augmented_batch_samples = reduce(operator.add, augmented_batch_samples)\n",
        "            augmented_batch_labels = reduce(operator.add, augmented_batch_labels)           \n",
        "            # Reshape input format\n",
        "            X_train = np.array(augmented_batch_samples)\n",
        "            X_train = X_train.reshape(X_train.shape[0], CROP_SIZE, CROP_SIZE, 1)\n",
        "            # Transform input to float and normalize\n",
        "            X_train = X_train.astype('float32')\n",
        "            X_train /= 255\n",
        "            # Encode y\n",
        "            y_train = np.array(augmented_batch_labels)\n",
        "            y_train = to_categorical(y_train, NUM_LABELS)\n",
        "            yield X_train, y_train\n",
        "\n",
        "\n",
        "train_generator = generate_data(X_train, y_train, BATCH_SIZE, 0.3)\n",
        "validation_generator = generate_data(X_val, y_val, BATCH_SIZE, 0.3)\n",
        "test_generator = generate_data(X_test, y_test, BATCH_SIZE, 0.1)\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "#config = tf.config.experimental\n",
        "#tf.Session(config = config)\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "tf.compat.v1.Session(config = config)\n",
        "\n",
        "\n",
        "def resize_image(img):\n",
        "    size = round(CROP_SIZE/2)\n",
        "    return tf.image.resize(img, [size, size])\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
        "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras import metrics\n",
        "model = Sequential()\n",
        "# Define network input shape\n",
        "model.add(ZeroPadding2D((1, 1), input_shape=(CROP_SIZE, CROP_SIZE, 1)))\n",
        "# Resize images to allow for easy computation\n",
        "model.add(Lambda(resize_image)) \n",
        "# CNN model - Building the model suggested in paper\n",
        "model.add(Convolution2D(filters= 32, kernel_size =(5,5), strides= (2, 2), padding='same', name='conv1'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool1'))\n",
        "model.add(Convolution2D(filters= 64, kernel_size =(3, 3), strides= (1, 1), padding='same', name='conv2'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool2'))\n",
        "model.add(Convolution2D(filters= 128, kernel_size =(3, 3), strides= (1, 1), padding='same', name='conv3'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool3'))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, name='dense1'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, name='dense2'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(NUM_LABELS, name='output'))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "# Create directory to save checkpoints at\n",
        "model_checkpoints_path = \"gdrive/My Drive/TCS RIO/model_checkpoints\"\n",
        "if not os.path.exists(model_checkpoints_path):\n",
        "    os.makedirs(model_checkpoints_path)\n",
        "# Save model after every epoch using checkpoints\n",
        "create_checkpoint = ModelCheckpoint(\n",
        "    filepath = \"gdrive/My Drive/TCS RIO/model_checkpoints/check-{epoch:02d}-{val_loss:.4f}.hdf5\",\n",
        "    verbose = 1,\n",
        "    save_best_only = False\n",
        ")\n",
        "# Fit model using generators\n",
        "history_object = model.fit_generator(\n",
        "    train_generator, \n",
        "    steps_per_epoch = 700 #round(len(X_train) / BATCH_SIZE),\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = round(len(X_val) / BATCH_SIZE),\n",
        "    epochs = 40,\n",
        "    verbose = 1,\n",
        "    callbacks = [create_checkpoint]\n",
        ")\n",
        "\n",
        "\n",
        "model_weights_path = \"gdrive/My Drive/TCS RIO/model_checkpoints/check-30-0.2748.hdf5\"\n",
        "if model_weights_path:\n",
        "    model.load_weights(model_weights_path)\n",
        "    scores = model.evaluate_generator(test_generator, steps=round(len(X_test)/BATCH_SIZE))\n",
        "    print(\"Accuracy: \", scores[1])\n",
        "else:\n",
        "    print(\"Set model weights file to load in the 'model_weights_path' variable\")\n",
        "\n",
        "\n",
        "# Load test data\n",
        "from random import sample \n",
        "images = []\n",
        "for filename in X_test[:50]:\n",
        "  im = Image.open(filename)\n",
        "  cur_width = im.size[0]\n",
        "  cur_height = im.size[1]\n",
        "  # print(\"Before Crop: \", cur_width, cur_height)\n",
        "  height_fac = CROP_SIZE / cur_height\n",
        "  new_width = int(cur_width * height_fac)\n",
        "  size = new_width, CROP_SIZE\n",
        "  # Resize so height = 113 while keeping aspect ratio\n",
        "  imresize = im.resize((size), Image.ANTIALIAS)  \n",
        "  now_width = imresize.size[0]\n",
        "  now_height = imresize.size[1]\n",
        "  # print(\"After Crop: \", now_width, now_height)\n",
        "  # Generate crops of size 113x113 from this resized image and keep random 10% of crops\n",
        "  avail_x_points = list(range(0, now_width - CROP_SIZE ))# total x start points are from 0 to width -113\n",
        "  # Pick random x%\n",
        "  factor = 0.1\n",
        "  pick_num = int(len(avail_x_points)*factor)\n",
        "  # print(\"Picked Number:\", pick_num)\n",
        "  random_startx = sample(avail_x_points,  pick_num)\n",
        "  for start in random_startx:\n",
        "      imcrop = imresize.crop((start, 0, start+CROP_SIZE, CROP_SIZE))\n",
        "      images.append(np.asarray(imcrop))\n",
        "  x_test = np.array(images)\n",
        "  x_test = x_test.reshape(x_test.shape[0], CROP_SIZE, CROP_SIZE, 1)\n",
        "  # convert to float and normalize\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_test /= 255\n",
        "  shuffle(x_test)\n",
        "print(x_test.shape)\n",
        "\n",
        "\n",
        "# Creating the predictive model\n",
        "predictions = model.predict(x_test, verbose=1)\n",
        "print(predictions.shape)\n",
        "predicted_writer = []\n",
        "for pred in predictions:\n",
        "  predicted_writer.append(np.argmax(pred))\n",
        "print(len(predicted_writer))\n",
        "\n",
        "\n",
        "# Mapping Test image with Trained image\n",
        "\n",
        "writer_number = 0\n",
        "total_images = 10\n",
        "counter = 0\n",
        "for i in range(len(predicted_writer)//10):\n",
        "  if predicted_writer[i] == writer_number:\n",
        "    image = x_test[i].squeeze()\n",
        "    plt.figure(figsize=(2,2))\n",
        "    plt.imshow(image, cmap ='gray')\n",
        "# print(x_test[1])\n",
        "image1 = x_test[0].squeeze()\n",
        "image2 = x_test[1].squeeze()\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(image1, cmap='gray');\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(image2, cmap='gray');\n"
      ]
    }
  ]
}